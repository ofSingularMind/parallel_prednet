{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET CONFIGURATION ARGUMENTS\n",
    "\n",
    "args = {\n",
    "    \"nt\": 1,\n",
    "    \"nb_epoch\": 250,\n",
    "    \"batch_size\": 1,\n",
    "    \"output_channels\": [3, 48, 96, 192],\n",
    "    \"num_P_CNN\": 1,\n",
    "    \"num_R_CLSTM\": 1,\n",
    "    \"num_passes\": 1,\n",
    "    \"pan_hierarchical\": False,\n",
    "    \"downscale_factor\": 4,\n",
    "    \"resize_images\": False,\n",
    "    \"train_proportion\": 0.7,\n",
    "    \"results_subdir\": \"dummy\",\n",
    "    \"dataset_weights\": \"various\",\n",
    "    \"data_subset_weights\": \"gen_ellipseV_crossH\",\n",
    "    \"dataset\": \"general_shape_static\",\n",
    "    \"data_subset\": \"general_ellipse_static_2nd_stage\",\n",
    "    \"data_subset_mode\": \"test\",\n",
    "    \"model_choice\": \"baseline\",\n",
    "    \"system\": \"laptop\",\n",
    "    \"reserialize_dataset\": False,\n",
    "    \"output_mode\": \"Error\"\n",
    "}\n",
    "args[\"results_subdir\"] = f\"interp_results/{args['dataset']}/{args['data_subset']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL\n",
    "\n",
    "import argparse\n",
    "from config import update_settings, get_settings\n",
    "from data_utils import serialize_dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "update_settings(args[\"system\"], args[\"dataset_weights\"], args[\"data_subset_weights\"], args[\"results_subdir\"])\n",
    "DATA_DIR, WEIGHTS_DIR, RESULTS_SAVE_DIR, LOG_DIR = get_settings()[\"dirs\"]\n",
    "data_dirs = [DATA_DIR, WEIGHTS_DIR, RESULTS_SAVE_DIR, LOG_DIR]\n",
    "if not os.path.exists(RESULTS_SAVE_DIR):\n",
    "    os.makedirs(RESULTS_SAVE_DIR)\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import hickle as hkl\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# or '2' to filter out INFO messages too\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from data_utils import SequenceGenerator, IntermediateEvaluations, create_dataset_from_serialized_generator, config_gpus \n",
    "%matplotlib tk\n",
    "import matplotlib.pyplot as plt\n",
    "# import addcopyfighandler\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import random\n",
    "\n",
    "# PICK MODEL\n",
    "if args[\"model_choice\"] == \"baseline\":\n",
    "    # Predict next frame along RGB channels only\n",
    "    if not args['pan_hierarchical']:\n",
    "        from PPN_models.PPN_Baseline import ParaPredNet\n",
    "    else:\n",
    "        from PPN_models.PPN_Baseline import ParaPredNet\n",
    "        print(\"Using Pan-Hierarchical Representation\")\n",
    "elif args[\"model_choice\"] == \"cl_delta\":\n",
    "    # Predict next frame and change from current frame\n",
    "    from PPN_models.PPN_CompLearning_Delta_Predictions import ParaPredNet\n",
    "elif args[\"model_choice\"] == \"cl_recon\":\n",
    "    # Predict current and next frame\n",
    "    from PPN_models.PPN_CompLearning_Recon_Predictions import ParaPredNet\n",
    "elif args[\"model_choice\"] == \"multi_channel\":\n",
    "    # Predict next frame along Disparity, Material Index, Object Index, \n",
    "    # Optical Flow, Motion Boundaries, and RGB channels all stacked together\n",
    "    assert args[\"dataset\"] == \"monkaa\" or args[\"dataset\"] == \"driving\", \"Multi-channel model only works with Monkaa or Driving dataset\"\n",
    "    from PPN_models.PPN_Multi_Channel import ParaPredNet\n",
    "    bottom_layer_output_channels = 7 # 1 Disparity, 3 Optical Flow, 3 RGB\n",
    "    args[\"output_channels\"][0] = bottom_layer_output_channels\n",
    "else:\n",
    "    raise ValueError(\"Invalid model choice\")\n",
    "\n",
    "# where weights are loaded prior to eval\n",
    "if (args[\"dataset_weights\"], args[\"data_subset_weights\"]) in [\n",
    "    (\"rolling_square\", \"single_rolling_square\"),\n",
    "    (\"rolling_circle\", \"single_rolling_circle\"),\n",
    "]:\n",
    "    # where weights will be loaded/saved\n",
    "    weights_file = os.path.join(WEIGHTS_DIR, f\"para_prednet_\"+args[\"data_subset\"]+\"_weights.hdf5\")\n",
    "elif (args[\"dataset_weights\"], args[\"data_subset_weights\"]) in [\n",
    "    (\"all_rolling\", \"single\"),\n",
    "    (\"all_rolling\", \"multi\")\n",
    "]:\n",
    "    # where weights will be loaded/saved\n",
    "    weights_file = os.path.join(WEIGHTS_DIR, f\"para_prednet_\"+args[\"dataset_weights\"]+\"_\"+args[\"data_subset_weights\"]+\"_weights.hdf5\")\n",
    "elif args[\"dataset_weights\"] in [\"all_rolling\", \"ball_collisions\", \"various\"]:\n",
    "    # where weights will be loaded/saved\n",
    "    weights_file = os.path.join(WEIGHTS_DIR, f\"para_prednet_\"+args[\"dataset_weights\"]+\"_\"+args[\"data_subset_weights\"]+\"_weights.hdf5\")\n",
    "else:\n",
    "    # where weights will be loaded/saved\n",
    "    weights_file = os.path.join(WEIGHTS_DIR, f\"para_prednet_\"+args[\"dataset_weights\"]+\"_weights.hdf5\")\n",
    "# weights_file = os.path.join(f\"/home/evalexii/Documents/Thesis/code/parallel_prednet/model_weights/{args['dataset_weights']}/{args['data_subset_weights']}\", f\"para_prednet_{args['data_subset_weights']}_weights.hdf5\")\n",
    "assert os.path.exists(weights_file), \"Weights file not found\"\n",
    "if args['dataset'] != args['dataset_weights']: \n",
    "    print(f\"WARNING: dataset ({args['dataset']}) and dataset_weights ({args['dataset_weights']}/{args['data_subset_weights']}) do not match - generalizing...\") \n",
    "else:\n",
    "    print(f\"OK: dataset ({args['dataset']}) and dataset_weights ({args['dataset_weights']}/{args['dataset_weights']}) match\") \n",
    "\n",
    "# Training parameters\n",
    "nt = args[\"nt\"]  # number of time steps\n",
    "batch_size = args[\"batch_size\"]  # 4\n",
    "output_channels = args[\"output_channels\"]\n",
    "\n",
    "# Define image shape\n",
    "if args[\"dataset\"] == \"kitti\":\n",
    "    original_im_shape = (128, 160, 3)\n",
    "    im_shape = original_im_shape\n",
    "elif args[\"dataset\"] == \"monkaa\" or args[\"dataset\"] == \"driving\":\n",
    "    original_im_shape = (540, 960, 3)\n",
    "    downscale_factor = args[\"downscale_factor\"]\n",
    "    im_shape = (original_im_shape[0] // downscale_factor, original_im_shape[1] // downscale_factor, 3)\n",
    "elif args[\"dataset\"] in [\"rolling_square\", \"rolling_circle\"]:\n",
    "    original_im_shape = (50, 100, 3)\n",
    "    downscale_factor = args[\"downscale_factor\"]\n",
    "    im_shape = (original_im_shape[0] // downscale_factor, original_im_shape[1] // downscale_factor, 3) if args[\"resize_images\"] else original_im_shape\n",
    "else:\n",
    "    original_im_shape = (50, 50, 3)\n",
    "    downscale_factor = args[\"downscale_factor\"]\n",
    "    im_shape = (original_im_shape[0] // downscale_factor, original_im_shape[1] // downscale_factor, 3) if args[\"resize_images\"] else original_im_shape\n",
    "\n",
    "print(f\"Working on dataset: {args['dataset']}\")\n",
    "\n",
    "# Create ParaPredNet\n",
    "if args[\"dataset\"] == \"kitti\":\n",
    "    # These are Kitti specific input shapes\n",
    "    inputs = (keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)))\n",
    "    PPN = ParaPredNet(args, im_height=im_shape[0], im_width=im_shape[1])  # [3, 48, 96, 192]\n",
    "    outputs = PPN(inputs)\n",
    "    PPN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "elif args[\"dataset\"] == \"monkaa\":\n",
    "    # These are Monkaa specific input shapes\n",
    "    inputs = (keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)),\n",
    "    )\n",
    "    PPN = ParaPredNet(args, im_height=im_shape[0], im_width=im_shape[1])  # [3, 48, 96, 192]\n",
    "    outputs = PPN(inputs)\n",
    "    PPN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "elif args[\"dataset\"] == \"driving\":\n",
    "    # These are driving specific input shapes\n",
    "    inputs = (keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)),\n",
    "    )\n",
    "    PPN = ParaPredNet(args, im_height=im_shape[0], im_width=im_shape[1])  # [3, 48, 96, 192]\n",
    "    outputs = PPN(inputs)\n",
    "    PPN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "else:\n",
    "    inputs = keras.Input(shape=(nt, im_shape[0], im_shape[1], 3))\n",
    "    PPN_layer = ParaPredNet(args, im_height=im_shape[0], im_width=im_shape[1])\n",
    "    PPN_layer.output_mode = \"Prediction\"\n",
    "    PPN_layer.continuous_eval = True\n",
    "    outputs = PPN_layer(inputs)\n",
    "    PPN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "resos = PPN.layers[-1].resolutions\n",
    "PPN.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "print(\"ParaPredNet compiled...\")\n",
    "PPN.build(input_shape=(None, nt) + im_shape)\n",
    "print(PPN.summary())\n",
    "num_layers = len(output_channels)  # number of layers in the architecture\n",
    "print(f\"{num_layers} PredNet layers with resolutions:\")\n",
    "for i in reversed(range(num_layers)):\n",
    "    print(f\"Layer {i+1}:  {resos[i][0]} x {resos[i][1]} x {output_channels[i]}\")\n",
    "\n",
    "# load previously saved weights\n",
    "try: \n",
    "    PPN.load_weights(weights_file)\n",
    "    print(\"Weights loaded successfully...\")\n",
    "except: \n",
    "    raise ValueError(\"Weights don't fit - exiting...\")\n",
    "\n",
    "# Load dataset - only working for animations\n",
    "try:\n",
    "    test_data = hkl.load(DATA_DIR + f\"{args['data_subset']}_{args['data_subset_mode']}.hkl\")[0]\n",
    "except:\n",
    "    png_paths = [DATA_DIR + f\"{args['dataset']}/frames/{args['data_subset']}/\"]\n",
    "    # png_paths = [DATA_DIR + f\"{args['dataset']}/frames/{args['data_subset']}_{args['data_subset_mode']}/\"]\n",
    "    serialize_dataset(data_dirs, pfm_paths=[], pgm_paths=[], png_paths=png_paths, dataset_name=args['data_subset'], test_data=True)\n",
    "    print(\"Dataset serialized...\")\n",
    "    test_data = hkl.load(DATA_DIR + f\"{args['data_subset']}_{args['data_subset_mode']}.hkl\")[0]\n",
    "print(\"Test data ready...\")\n",
    "\n",
    "td_len = test_data.shape[0]\n",
    "ppn = PPN.layers[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA COLLECTION CODE - AGGREGATION OR STATE DATA\n",
    "\n",
    "collection_mode = \"agg\" # \"agg\" or \"state\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR + f\"/{collection_mode}_data_{args['data_subset']}_{args['data_subset_mode']}.hkl\"):\n",
    "    print(f\"Collecting {collection_mode} data...\")\n",
    "\n",
    "    # Assuming initialization and data loading is done here\n",
    "    start = 0\n",
    "    stop = td_len - 1\n",
    "    num_samples = td_len - 1\n",
    "    sample_shape = (1, 1, *test_data.shape[1:])\n",
    "\n",
    "    # Initialize lists to store intermediate data for each layer\n",
    "    R_states_list = [[] for _ in ppn.predlayers]\n",
    "    P_states_list = [[] for _ in ppn.predlayers]\n",
    "\n",
    "    # initialize lists to hold global max pooled states\n",
    "    R_state_maxes = [[] for _ in ppn.predlayers]\n",
    "    P_state_maxes = [[] for _ in ppn.predlayers]\n",
    "\n",
    "    # initialize lists to hold top k% of channels by weight\n",
    "    R_agg_mtx = [np.zeros((ppn.predlayers[j].output_channels)) for j in range(len(ppn.predlayers))]\n",
    "    P_agg_mtx = [np.zeros((ppn.predlayers[j].output_channels)) for j in range(len(ppn.predlayers))]\n",
    "\n",
    "    # Collect all states\n",
    "    indices = np.random.permutation(range(start, stop))\n",
    "    for it, i in enumerate(indices[:num_samples]):\n",
    "        ppn.init_layer_states()\n",
    "        ground_truth_image = np.reshape(test_data[i], sample_shape)\n",
    "        predicted_image = ppn(ground_truth_image)\n",
    "        predicted_image = ppn(ground_truth_image)\n",
    "        # ground_truth_image = np.reshape(test_data[i+1], sample_shape)\n",
    "        # predicted_image = ppn(ground_truth_image)\n",
    "        \n",
    "        # # UNCOMMENT TO COLLECT ALL STATE DATA\n",
    "        # for j in range(len(ppn.predlayers)):\n",
    "        #     R_states_list[j].append(ppn.predlayers[j].states[\"R\"][0])\n",
    "        #     P_states_list[j].append(ppn.predlayers[j].states[\"P\"][0])\n",
    "\n",
    "        # # UNCOMMENT TO COLLECT GLOBAL MAX POOLED STATES\n",
    "        # for j in range(len(ppn.predlayers)):\n",
    "        #     R_state_maxes[j].append(np.max(ppn.predlayers[j].states[\"R\"][0], axis=(0,1)))\n",
    "        #     P_state_maxes[j].append(np.max(ppn.predlayers[j].states[\"P\"][0], axis=(0,1)))\n",
    "\n",
    "        # UNCOMMMENT TO COLLENT AGGREGATION MATRICES\n",
    "        # Aggregate top k% of channels by weight\n",
    "        # Need a vector of length equal to the number of channels in the layer\n",
    "        k = 0.1\n",
    "        for j in range(len(ppn.predlayers)):\n",
    "            R_max_pooled = np.max(ppn.predlayers[j].states[\"R\"][0], axis=(0,1))\n",
    "            P_max_pooled = np.max(ppn.predlayers[j].states[\"P\"][0], axis=(0,1))\n",
    "            R_norm = R_max_pooled / np.sum(R_max_pooled)\n",
    "            P_norm = P_max_pooled / np.sum(P_max_pooled)\n",
    "            R_sorted = np.argsort(R_norm)[::-1]\n",
    "            P_sorted = np.argsort(P_norm)[::-1]\n",
    "            R_sum = 0\n",
    "            P_sum = 0\n",
    "            R_indices = []\n",
    "            P_indices = []\n",
    "            for l in range(len(R_sorted)):\n",
    "                if R_sum <= k:\n",
    "                    R_sum += R_norm[R_sorted[l]]\n",
    "                if (R_sum <= k) or (l == 0):\n",
    "                    R_indices.append(R_sorted[l])\n",
    "                else:\n",
    "                    break\n",
    "            for l in range(len(R_sorted)):\n",
    "                if P_sum <= k:\n",
    "                    P_sum += P_norm[P_sorted[l]]\n",
    "                if (P_sum <= k) or (l == 0):\n",
    "                    P_indices.append(R_sorted[l])\n",
    "                else:\n",
    "                    break\n",
    "            R_agg_mtx[j][R_indices] += 1\n",
    "            P_agg_mtx[j][P_indices] += 1\n",
    "\n",
    "    print(\"Done...\")\n",
    "    # Save intermediate state data\n",
    "    print(f\"Saving {collection_mode} data...\")\n",
    "    hkl.dump([R_agg_mtx, P_agg_mtx], DATA_DIR + f\"/{collection_mode}_data_{args['data_subset']}_{args['data_subset_mode']}.hkl\")\n",
    "    print(\"Done...\")\n",
    "\n",
    "else:\n",
    "    print(f\"{collection_mode} data for '{args['data_subset']}_{args['data_subset_mode']}' already exists...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD AND PLOT AGGREGATION MATRICES AND DIFFERENCE MATRICES\n",
    "\n",
    "agg_datasets = [\n",
    "    DATA_DIR + f\"/agg_data_general_cross_static_2nd_stage_test.hkl\",\n",
    "    DATA_DIR + f\"/agg_data_general_ellipse_static_2nd_stage_test.hkl\",\n",
    "]\n",
    "\n",
    "# Post-process all states after collecting\n",
    "R_agg_mtxs = [[] for _ in agg_datasets]\n",
    "P_agg_mtxs = [[] for _ in agg_datasets]\n",
    "\n",
    "for i in range(len(agg_datasets)):\n",
    "    R_agg_mtxs[i], P_agg_mtxs[i] = hkl.load(agg_datasets[i])\n",
    "\n",
    "# Plot the aggregate matrices\n",
    "fig, axs = plt.subplots(4, 2, figsize=(10, 10))\n",
    "colors = ['blue','orange']\n",
    "for i in range(len(agg_datasets)):\n",
    "    for j in range(len(ppn.predlayers)):\n",
    "        indices = range(len(R_agg_mtxs[i][j]))\n",
    "        axs[j,0].bar(indices,R_agg_mtxs[i][j], alpha=0.7, color=colors[i])\n",
    "        axs[j,1].bar(indices,P_agg_mtxs[i][j], alpha=0.7, color=colors[i])\n",
    "        axs[j,0].set_title(f\"R Agg, Layer {j+1}\")\n",
    "        axs[j,1].set_title(f\"P Agg, Layer {j+1}\")\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Find the difference matrices from the two datasets\n",
    "R_diffs = [[] for _ in range(len(agg_datasets))]\n",
    "P_diffs = [[] for _ in range(len(agg_datasets))]\n",
    "for i in range(len(agg_datasets)):\n",
    "    for j in range(len(ppn.predlayers)):\n",
    "        R_diff = R_agg_mtxs[i][j] - R_agg_mtxs[1-i][j]\n",
    "        R_diff[R_diff < 0] = 0\n",
    "        P_diff = P_agg_mtxs[i][j] - P_agg_mtxs[1-i][j]\n",
    "        P_diff[P_diff < 0] = 0\n",
    "        R_diffs[i].append(R_diff)\n",
    "        P_diffs[i].append(P_diff)\n",
    "\n",
    "# Plot the difference matrices\n",
    "fig, axs = plt.subplots(4, 2, figsize=(10, 10))\n",
    "colors = ['blue','orange']\n",
    "for i in range(len(agg_datasets)):\n",
    "    for j in range(len(ppn.predlayers)):\n",
    "        indices = range(len(R_diffs[i][j]))\n",
    "        axs[j,0].bar(indices,R_diffs[i][j], alpha=0.7, color=colors[i])\n",
    "        axs[j,1].bar(indices,P_diffs[i][j], alpha=0.7, color=colors[i])\n",
    "        axs[j,0].set_title(f\"R Aggregate Differences, Layer {j+1}\")\n",
    "        axs[j,1].set_title(f\"P Aggregate Differences, Layer {j+1}\")\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD STATE MAXES\n",
    "\n",
    "state_datasets = [\n",
    "    DATA_DIR + f\"/state_data_general_cross_static_2nd_stage_test.hkl\",\n",
    "    DATA_DIR + f\"/state_data_general_ellipse_static_2nd_stage_test.hkl\",\n",
    "]\n",
    "\n",
    "# Post-process all states after collecting\n",
    "R_state_maxes = [[] for _ in state_datasets]\n",
    "P_state_maxes = [[] for _ in state_datasets]\n",
    "R_max_indices = [[] for _ in state_datasets]\n",
    "P_max_indices = [[] for _ in state_datasets]\n",
    "\n",
    "# if saved datasets are state maxes\n",
    "for i in range(len(state_datasets)):\n",
    "    R_state_maxes[i], P_state_maxes[i] = hkl.load(state_datasets[i])\n",
    "    for j in range(len(ppn.predlayers)):\n",
    "        R_state_maxes[i][j] = np.array(R_state_maxes[i][j])\n",
    "        P_state_maxes[i][j] = np.array(P_state_maxes[i][j])\n",
    "\n",
    "# # if saved datasets are state tensors\n",
    "# for i in range(len(state_datasets)):\n",
    "#     R_states_list, P_states_list = hkl.load(state_datasets[i])\n",
    "#     for j in range(len(ppn.predlayers)):\n",
    "#         R_states = np.array(R_states_list[j])\n",
    "#         P_states = np.array(P_states_list[j])\n",
    "        \n",
    "#         # Max pooling across all samples for each layer\n",
    "#         R_max_pooled = np.max(R_states, axis=(1, 2))  # Max pooling across spatial dimensions\n",
    "#         P_max_pooled = np.max(P_states, axis=(1, 2))\n",
    "        \n",
    "#         R_state_maxes[i].append(R_max_pooled)\n",
    "#         P_state_maxes[i].append(P_max_pooled)\n",
    "        \n",
    "#         # Find top-k indices\n",
    "#         num_filters = R_states.shape[-1]\n",
    "#         top_k = num_filters // 3\n",
    "        \n",
    "#         R_max_indices[i].append(np.argsort(R_max_pooled, axis=1)[:,-top_k:])\n",
    "#         P_max_indices[i].append(np.argsort(P_max_pooled, axis=1)[:,-top_k:])\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT ALL LAYER STATE MAXES\n",
    "\n",
    "# Calculate mean, std of max activations per layer, plot error bars\n",
    "# discard activations with std > 0.1\n",
    "# then plot top 10 activations with x-axis as filter index\n",
    "std_max = 0.1\n",
    "top_k = 10\n",
    "fig, axs = plt.subplots(2, 4, figsize=(50, 50))\n",
    "labels = [\"Cross-Right\", \"Ellipse-Down\"]\n",
    "for k in range(len(state_datasets)):\n",
    "    for i in range(len(ppn.predlayers)):\n",
    "        R_mean_o = np.mean(R_state_maxes[k][i], axis=0)\n",
    "        R_std_o = np.std(R_state_maxes[k][i], axis=0)\n",
    "        R_indices_o = np.arange(len(R_mean_o))\n",
    "        R_mean_i = R_mean_o[R_std_o < std_max]\n",
    "        R_mean = R_mean_i[np.argsort(R_mean_i)[-top_k:]]\n",
    "        R_std_i = R_std_o[R_std_o < std_max]\n",
    "        R_std = R_std_i[np.argsort(R_mean_i)[-top_k:]]\n",
    "        R_indices_i = R_indices_o[R_std_o < std_max]\n",
    "        R_indices = R_indices_i[np.argsort(R_mean_i)[-top_k:]]\n",
    "        axs[0,i].errorbar(R_indices, R_mean, yerr=R_std, fmt='o', label=labels[k])\n",
    "        P_mean_o = np.mean(P_state_maxes[k][i], axis=0)\n",
    "        P_std_o = np.std(P_state_maxes[k][i], axis=0)\n",
    "        P_indices_o = np.arange(len(P_mean_o))\n",
    "        P_mean_i = P_mean_o[P_std_o < std_max]\n",
    "        P_mean = P_mean_i[np.argsort(P_mean_i)[-top_k:]]\n",
    "        P_std_i = P_std_o[P_std_o < std_max]\n",
    "        P_std = P_std_i[np.argsort(P_mean_i)[-top_k:]]\n",
    "        P_indices_i = P_indices_o[P_std_o < std_max]\n",
    "        P_indices = P_indices_i[np.argsort(P_mean_i)[-top_k:]]\n",
    "        axs[1,i].errorbar(P_indices, P_mean, yerr=P_std, fmt='o', label=labels[k])\n",
    "        # axs[0,i].set_ylim(-0.1, 1.1)\n",
    "        # axs[1,i].set_ylim(-0.1, 1.1)\n",
    "        axs[0,i].set_title(f\"R-State Layer {i+1}\")\n",
    "        axs[1,i].set_title(f\"P-State Layer {i+1}\")\n",
    "        axs[0,i].set_xlabel(\"Channel Index\")\n",
    "        axs[1,i].set_xlabel(\"Channel Index\")\n",
    "        axs[0,i].legend()\n",
    "        axs[1,i].legend()\n",
    "        \n",
    "fig.suptitle(f\"Top {top_k} R- / P-State Max Channel-Values with STD < {std_max}\")\n",
    "# axs[1].set_title(f\"Top {top_k} P State Max Values with STD < {std_max}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT SINGLE LAYER STATE MAXES\n",
    "\n",
    "# Calculate mean, std of max activations per layer, plot error bars\n",
    "# discard activations with std > 0.1\n",
    "# then plot top 10 activations with x-axis as filter index\n",
    "std_max = 1\n",
    "top_k = 200\n",
    "fig, axs = plt.subplots(2, 1, figsize=(50, 50))\n",
    "labels = [\"Cross-Right\", \"Ellipse-Down\"]\n",
    "i = 3 # layer index (L-1)\n",
    "for k in range(len(state_datasets)):\n",
    "\n",
    "    R_mean_o = np.mean(R_state_maxes[k][i], axis=0)\n",
    "    R_std_o = np.std(R_state_maxes[k][i], axis=0)\n",
    "    R_indices_o = np.arange(len(R_mean_o))\n",
    "    R_mean_i = R_mean_o[R_std_o < std_max]\n",
    "    R_mean = R_mean_i[np.argsort(R_mean_i)[-top_k:]]\n",
    "    R_std_i = R_std_o[R_std_o < std_max]\n",
    "    R_std = R_std_i[np.argsort(R_mean_i)[-top_k:]]\n",
    "    R_indices_i = R_indices_o[R_std_o < std_max]\n",
    "    R_indices = R_indices_i[np.argsort(R_mean_i)[-top_k:]]\n",
    "    P_mean_o = np.mean(P_state_maxes[k][i], axis=0)\n",
    "    P_std_o = np.std(P_state_maxes[k][i], axis=0)\n",
    "    P_indices_o = np.arange(len(P_mean_o))\n",
    "    P_mean_i = P_mean_o[P_std_o < std_max]\n",
    "    P_mean = P_mean_i[np.argsort(P_mean_i)[-top_k:]]\n",
    "    P_std_i = P_std_o[P_std_o < std_max]\n",
    "    P_std = P_std_i[np.argsort(P_mean_i)[-top_k:]]\n",
    "    P_indices_i = P_indices_o[P_std_o < std_max]\n",
    "    P_indices = P_indices_i[np.argsort(P_mean_i)[-top_k:]]\n",
    "    axs[0].errorbar(R_indices, R_mean, yerr=R_std, fmt='o', label=labels[k])\n",
    "    axs[1].errorbar(P_indices, P_mean, yerr=P_std, fmt='o', label=labels[k])\n",
    "    axs[0].set_ylim(-0.1, 1.1)\n",
    "    axs[1].set_ylim(-0.1, 1.1)\n",
    "    axs[0].set_title(f\"R-State Layer {i+1}\")\n",
    "    axs[1].set_title(f\"P-State Layer {i+1}\")\n",
    "    axs[0].set_xlabel(\"Channel Index\")\n",
    "    axs[1].set_xlabel(\"Channel Index\")\n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "fig.suptitle(f\"Top {top_k} R- / P-State Max Channel-Values with STD < {std_max}\")\n",
    "# axs[1].set_title(f\"Top {top_k} P State Max Values with STD < {std_max}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD STATE MAX EXTRACTION CODE\n",
    "\n",
    "# iterate through whole test dataset pulling out filters from each layer\n",
    "ppn = PPN.layers[-1]\n",
    "start = 0\n",
    "stop = td_len-1\n",
    "num_samples = td_len-1\n",
    "# initialize lists to hold global max pooled states\n",
    "R_state_maxes = [None]*len(ppn.predlayers)\n",
    "P_state_maxes = [None]*len(ppn.predlayers)\n",
    "R_max_indices = [None]*len(ppn.predlayers)\n",
    "P_max_indices = [None]*len(ppn.predlayers)\n",
    "all_R_states_maxes = None\n",
    "all_P_states_maxes = None\n",
    "all_R_max_indices = None\n",
    "all_P_max_indices = None\n",
    "for it, i in enumerate(random.sample(range(start, stop), num_samples)):\n",
    "    print(f\"Sample {it+1}/{num_samples}...\")\n",
    "    # if i > 0: break\n",
    "    # manually initialize PPN layer states\n",
    "    ppn.init_layer_states()\n",
    "    # run image through twice to get activation that captures class-recognition\n",
    "    # not necessary to feed second sequence image in, as the image only contributes to bottom-up error, but nonetheless...\n",
    "    ground_truth_image = np.reshape(test_data[i], (1, 1, *test_data.shape[1:]))\n",
    "    predicted_image = ppn(ground_truth_image)\n",
    "    ground_truth_image = np.reshape(test_data[i+1], (1, 1, *test_data.shape[1:]))\n",
    "    predicted_image = ppn(ground_truth_image)\n",
    "\n",
    "    # add all state tensors to lists\n",
    "    R_states = []\n",
    "    P_states = []\n",
    "    for j in range(len(ppn.predlayers)):\n",
    "        R_states.append(ppn.predlayers[j].states[\"R\"][0])\n",
    "        P_states.append(ppn.predlayers[j].states[\"P\"][0])\n",
    "    \n",
    "    # perform global max pooling on each layer's R and P states\n",
    "    for j in range(len(ppn.predlayers)):\n",
    "        R_state_maxes[j] = np.expand_dims(np.max(R_states[j], axis=(0,1)), axis=0) if R_state_maxes[j] is None else np.concatenate((R_state_maxes[j], np.expand_dims(np.max(R_states[j], axis=(0,1)), axis=0)), axis=0)\n",
    "        P_state_maxes[j] = np.expand_dims(np.max(P_states[j], axis=(0,1)), axis=0) if P_state_maxes[j] is None else np.concatenate((P_state_maxes[j], np.expand_dims(np.max(P_states[j], axis=(0,1)), axis=0)), axis=0)\n",
    "\n",
    "        num_filters = R_states[j].shape[-1]\n",
    "        assert R_states[j].shape[-1] == P_states[j].shape[-1]\n",
    "        top_k = int(num_filters/3)\n",
    "\n",
    "        R_max_indices[j] = np.expand_dims(np.argsort(R_state_maxes[j][-1])[-top_k:], axis=0) if R_max_indices[j] is None else np.concatenate((R_max_indices[j], np.expand_dims(np.argsort(R_state_maxes[j][-1])[:top_k], axis=0)), axis=0)\n",
    "        P_max_indices[j] = np.expand_dims(np.argsort(P_state_maxes[j][-1])[-top_k:], axis=0) if P_max_indices[j] is None else np.concatenate((P_max_indices[j], np.expand_dims(np.argsort(P_state_maxes[j][-1])[:top_k], axis=0)), axis=0)\n",
    "    \n",
    "    # also find pan-hierarchical distributed representations\n",
    "    all_R_states_maxes = np.expand_dims(np.concatenate([j[-1] for j in R_state_maxes], axis=0), axis=0) if all_R_states_maxes is None else np.concatenate((all_R_states_maxes, np.expand_dims(np.concatenate([j[-1] for j in R_state_maxes], axis=0), axis=0)), axis=0)\n",
    "    all_P_states_maxes = np.expand_dims(np.concatenate([j[-1] for j in P_state_maxes], axis=0), axis=0) if all_P_states_maxes is None else np.concatenate((all_P_states_maxes, np.expand_dims(np.concatenate([j[-1] for j in P_state_maxes], axis=0), axis=0)), axis=0)\n",
    "    all_R_max_indices = np.expand_dims(np.argsort(all_R_states_maxes[-1])[-10:], axis=0) if all_R_max_indices is None else np.concatenate((all_R_max_indices, np.expand_dims(np.argsort(all_R_states_maxes[-1])[:10], axis=0)), axis=0)\n",
    "    all_P_max_indices = np.expand_dims(np.argsort(all_P_states_maxes[-1])[-10:], axis=0) if all_P_max_indices is None else np.concatenate((all_P_max_indices, np.expand_dims(np.argsort(all_P_states_maxes[-1])[:10], axis=0)), axis=0)\n",
    "\n",
    "    # # plot filter weights for each layer\n",
    "    # num_layers = len(ppn.predlayers)\n",
    "    # for layer in ppn.predlayers:\n",
    "    #     for p_c_layer in layer.prediction.conv_layers:\n",
    "    #         for weights in p_c_layer.trainable_weights:\n",
    "    #             if len(weights.shape) == 1: continue\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
