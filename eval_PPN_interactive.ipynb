{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: dataset (general_ellipse_vertical) and dataset_weights (various/gen_ellipseV_crossH) do not match - generalizing...\n",
      "Working on dataset: general_ellipse_vertical\n",
      "ParaPredNet compiled...\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1, 50, 50, 3)]    0         \n",
      "                                                                 \n",
      " para_pred_net (ParaPredNet  (1, 1, 50, 50, 3)         6915948   \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6915948 (26.38 MB)\n",
      "Trainable params: 6915948 (26.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "4 PredNet layers with resolutions:\n",
      "Layer 4:  6 x 6 x 192\n",
      "Layer 3:  12 x 12 x 96\n",
      "Layer 2:  25 x 25 x 48\n",
      "Layer 1:  50 x 50 x 3\n",
      "Weights loaded successfully...\n",
      "Test data ready...\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from config import update_settings, get_settings\n",
    "from data_utils import serialize_dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "args = {\n",
    "    \"nt\": 1,\n",
    "    \"nb_epoch\": 250,\n",
    "    \"batch_size\": 1,\n",
    "    \"output_channels\": [3, 48, 96, 192],\n",
    "    \"num_P_CNN\": 1,\n",
    "    \"num_R_CLSTM\": 1,\n",
    "    \"num_passes\": 1,\n",
    "    \"pan_hierarchical\": False,\n",
    "    \"downscale_factor\": 4,\n",
    "    \"resize_images\": False,\n",
    "    \"train_proportion\": 0.7,\n",
    "    \"results_subdir\": f\"{str(datetime.now())}\",\n",
    "    \"dataset_weights\": \"various\",\n",
    "    \"data_subset_weights\": \"gen_ellipseV_crossH\",\n",
    "    \"dataset\": \"general_ellipse_vertical\",\n",
    "    \"data_subset\": \"general_ellipse_vertical\",\n",
    "    \"data_subset_mode\": \"train\",\n",
    "    \"model_choice\": \"baseline\",\n",
    "    \"system\": \"laptop\",\n",
    "    \"reserialize_dataset\": False,\n",
    "    \"output_mode\": \"Error\"\n",
    "}\n",
    "\n",
    "\n",
    "update_settings(args[\"system\"], args[\"dataset_weights\"], args[\"data_subset_weights\"], args[\"results_subdir\"])\n",
    "DATA_DIR, WEIGHTS_DIR, RESULTS_SAVE_DIR, LOG_DIR = get_settings()[\"dirs\"]\n",
    "data_dirs = [DATA_DIR, WEIGHTS_DIR, RESULTS_SAVE_DIR, LOG_DIR]\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import hickle as hkl\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# or '2' to filter out INFO messages too\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from data_utils import SequenceGenerator, IntermediateEvaluations, create_dataset_from_serialized_generator, config_gpus \n",
    "%matplotlib tk\n",
    "import matplotlib.pyplot as plt\n",
    "# import addcopyfighandler\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# PICK MODEL\n",
    "if args[\"model_choice\"] == \"baseline\":\n",
    "    # Predict next frame along RGB channels only\n",
    "    if not args['pan_hierarchical']:\n",
    "        from PPN_models.PPN_Baseline import ParaPredNet\n",
    "    else:\n",
    "        from PPN_models.PPN_Baseline import ParaPredNet\n",
    "        print(\"Using Pan-Hierarchical Representation\")\n",
    "elif args[\"model_choice\"] == \"cl_delta\":\n",
    "    # Predict next frame and change from current frame\n",
    "    from PPN_models.PPN_CompLearning_Delta_Predictions import ParaPredNet\n",
    "elif args[\"model_choice\"] == \"cl_recon\":\n",
    "    # Predict current and next frame\n",
    "    from PPN_models.PPN_CompLearning_Recon_Predictions import ParaPredNet\n",
    "elif args[\"model_choice\"] == \"multi_channel\":\n",
    "    # Predict next frame along Disparity, Material Index, Object Index, \n",
    "    # Optical Flow, Motion Boundaries, and RGB channels all stacked together\n",
    "    assert args[\"dataset\"] == \"monkaa\" or args[\"dataset\"] == \"driving\", \"Multi-channel model only works with Monkaa or Driving dataset\"\n",
    "    from PPN_models.PPN_Multi_Channel import ParaPredNet\n",
    "    bottom_layer_output_channels = 7 # 1 Disparity, 3 Optical Flow, 3 RGB\n",
    "    args[\"output_channels\"][0] = bottom_layer_output_channels\n",
    "else:\n",
    "    raise ValueError(\"Invalid model choice\")\n",
    "\n",
    "# where weights are loaded prior to eval\n",
    "if (args[\"dataset_weights\"], args[\"data_subset_weights\"]) in [\n",
    "    (\"rolling_square\", \"single_rolling_square\"),\n",
    "    (\"rolling_circle\", \"single_rolling_circle\"),\n",
    "]:\n",
    "    # where weights will be loaded/saved\n",
    "    weights_file = os.path.join(WEIGHTS_DIR, f\"para_prednet_\"+args[\"data_subset\"]+\"_weights.hdf5\")\n",
    "elif (args[\"dataset_weights\"], args[\"data_subset_weights\"]) in [\n",
    "    (\"all_rolling\", \"single\"),\n",
    "    (\"all_rolling\", \"multi\")\n",
    "]:\n",
    "    # where weights will be loaded/saved\n",
    "    weights_file = os.path.join(WEIGHTS_DIR, f\"para_prednet_\"+args[\"dataset_weights\"]+\"_\"+args[\"data_subset_weights\"]+\"_weights.hdf5\")\n",
    "elif args[\"dataset_weights\"] in [\"all_rolling\", \"ball_collisions\", \"various\"]:\n",
    "    # where weights will be loaded/saved\n",
    "    weights_file = os.path.join(WEIGHTS_DIR, f\"para_prednet_\"+args[\"dataset_weights\"]+\"_\"+args[\"data_subset_weights\"]+\"_weights.hdf5\")\n",
    "else:\n",
    "    # where weights will be loaded/saved\n",
    "    weights_file = os.path.join(WEIGHTS_DIR, f\"para_prednet_\"+args[\"dataset_weights\"]+\"_weights.hdf5\")\n",
    "# weights_file = os.path.join(f\"/home/evalexii/Documents/Thesis/code/parallel_prednet/model_weights/{args['dataset_weights']}/{args['data_subset_weights']}\", f\"para_prednet_{args['data_subset_weights']}_weights.hdf5\")\n",
    "assert os.path.exists(weights_file), \"Weights file not found\"\n",
    "if args['dataset'] != args['dataset_weights']: \n",
    "    print(f\"WARNING: dataset ({args['dataset']}) and dataset_weights ({args['dataset_weights']}/{args['data_subset_weights']}) do not match - generalizing...\") \n",
    "else:\n",
    "    print(f\"OK: dataset ({args['dataset']}) and dataset_weights ({args['dataset_weights']}/{args['dataset_weights']}) match\") \n",
    "\n",
    "# Training parameters\n",
    "nt = args[\"nt\"]  # number of time steps\n",
    "batch_size = args[\"batch_size\"]  # 4\n",
    "output_channels = args[\"output_channels\"]\n",
    "\n",
    "# Define image shape\n",
    "if args[\"dataset\"] == \"kitti\":\n",
    "    original_im_shape = (128, 160, 3)\n",
    "    im_shape = original_im_shape\n",
    "elif args[\"dataset\"] == \"monkaa\" or args[\"dataset\"] == \"driving\":\n",
    "    original_im_shape = (540, 960, 3)\n",
    "    downscale_factor = args[\"downscale_factor\"]\n",
    "    im_shape = (original_im_shape[0] // downscale_factor, original_im_shape[1] // downscale_factor, 3)\n",
    "elif args[\"dataset\"] in [\"rolling_square\", \"rolling_circle\"]:\n",
    "    original_im_shape = (50, 100, 3)\n",
    "    downscale_factor = args[\"downscale_factor\"]\n",
    "    im_shape = (original_im_shape[0] // downscale_factor, original_im_shape[1] // downscale_factor, 3) if args[\"resize_images\"] else original_im_shape\n",
    "else:\n",
    "    original_im_shape = (50, 50, 3)\n",
    "    downscale_factor = args[\"downscale_factor\"]\n",
    "    im_shape = (original_im_shape[0] // downscale_factor, original_im_shape[1] // downscale_factor, 3) if args[\"resize_images\"] else original_im_shape\n",
    "\n",
    "print(f\"Working on dataset: {args['dataset']}\")\n",
    "\n",
    "# Create ParaPredNet\n",
    "if args[\"dataset\"] == \"kitti\":\n",
    "    # These are Kitti specific input shapes\n",
    "    inputs = (keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)))\n",
    "    PPN = ParaPredNet(args, im_height=im_shape[0], im_width=im_shape[1])  # [3, 48, 96, 192]\n",
    "    outputs = PPN(inputs)\n",
    "    PPN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "elif args[\"dataset\"] == \"monkaa\":\n",
    "    # These are Monkaa specific input shapes\n",
    "    inputs = (keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)),\n",
    "    )\n",
    "    PPN = ParaPredNet(args, im_height=im_shape[0], im_width=im_shape[1])  # [3, 48, 96, 192]\n",
    "    outputs = PPN(inputs)\n",
    "    PPN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "elif args[\"dataset\"] == \"driving\":\n",
    "    # These are driving specific input shapes\n",
    "    inputs = (keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)),\n",
    "    )\n",
    "    PPN = ParaPredNet(args, im_height=im_shape[0], im_width=im_shape[1])  # [3, 48, 96, 192]\n",
    "    outputs = PPN(inputs)\n",
    "    PPN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "else:\n",
    "    # These are rolling_square specific input shapes\n",
    "    inputs = keras.Input(shape=(nt, im_shape[0], im_shape[1], 3))\n",
    "    PPN_layer = ParaPredNet(args, im_height=im_shape[0], im_width=im_shape[1])\n",
    "    PPN_layer.output_mode = \"Prediction\"\n",
    "    PPN_layer.continuous_eval = True\n",
    "    outputs = PPN_layer(inputs)\n",
    "    PPN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "resos = PPN.layers[-1].resolutions\n",
    "PPN.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "print(\"ParaPredNet compiled...\")\n",
    "PPN.build(input_shape=(None, nt) + im_shape)\n",
    "print(PPN.summary())\n",
    "num_layers = len(output_channels)  # number of layers in the architecture\n",
    "print(f\"{num_layers} PredNet layers with resolutions:\")\n",
    "for i in reversed(range(num_layers)):\n",
    "    print(f\"Layer {i+1}:  {resos[i][0]} x {resos[i][1]} x {output_channels[i]}\")\n",
    "\n",
    "# load previously saved weights\n",
    "try: \n",
    "    PPN.load_weights(weights_file)\n",
    "    print(\"Weights loaded successfully...\")\n",
    "except: \n",
    "    raise ValueError(\"Weights don't fit - exiting...\")\n",
    "\n",
    "# Load dataset - only working for animations\n",
    "try:\n",
    "    test_data = hkl.load(DATA_DIR + f\"{args['data_subset']}_{args['data_subset_mode']}.hkl\")[0]\n",
    "except:\n",
    "    png_paths = [DATA_DIR + f\"{args['data_subset']}/frames/{args['data_subset']}_{args['data_subset_mode']}/\"]\n",
    "    serialize_dataset(data_dirs, pfm_paths=[], pgm_paths=[], png_paths=png_paths, dataset_name=args['data_subset'], test_data=True)\n",
    "    print(\"Dataset serialized...\")\n",
    "    test_data = hkl.load(DATA_DIR + f\"{args['data_subset']}_{args['data_subset_mode']}.hkl\")[0]\n",
    "print(\"Test data ready...\")\n",
    "\n",
    "td_len = test_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually initialize PPN layer states\n",
    "PPN.layers[-1].init_layer_states()\n",
    "\n",
    "# dataset_iter = iter(test_dataset)\n",
    "# fig, axs = plt.subplots(1, 3, figsize=(20, 4))\n",
    "# plt.show(block=False)\n",
    "rg_colormap = LinearSegmentedColormap.from_list('custom_cmap', [(0, 'red'), (0.5, 'black'), (1, 'green')])\n",
    "\n",
    "# test_data = np.reshape(test_data, (batch_size, td_len, im_shape[0], im_shape[1], 3))\n",
    "\n",
    "idx = 1100\n",
    "# plot the next ten frames starting from idx\n",
    "fig, axs = plt.subplots(5, 10, figsize=(20, 4))\n",
    "for i in range(5):\n",
    "    for j in range(10):\n",
    "        axs[i, j].axis(\"off\")\n",
    "        axs[i, j].imshow(test_data[idx+j+i*10])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# for i in range(td_len):\n",
    "#     if i < 98: continue\n",
    "#     # ground_truth_image = next(dataset_iter)[0]\n",
    "#     print(f\"Iteration {i+1}/{td_len}\")\n",
    "#     ground_truth_image = np.reshape(test_data[i], (1, 1, *test_data.shape[1:]))\n",
    "#     predicted_image = PPN.layers[-1](ground_truth_image)\n",
    "#     error_image = ground_truth_image - predicted_image\n",
    "#     error_image_grey = np.mean(error_image, axis=-1, keepdims=True)\n",
    "#     mse = np.mean(error_image**2)\n",
    "\n",
    "#     # clear the axes\n",
    "#     axs[0].cla()\n",
    "#     axs[1].cla()\n",
    "#     axs[2].cla()\n",
    "\n",
    "#     # print the two images side-by-side\n",
    "#     axs[0].imshow(ground_truth_image[0,0,...])\n",
    "#     axs[1].imshow(predicted_image[0,0,...])\n",
    "#     axs[2].imshow(error_image_grey[0,0,...], cmap=rg_colormap)\n",
    "\n",
    "#     # add titles\n",
    "#     axs[0].set_title(\"Ground Truth\")\n",
    "#     axs[1].set_title(\"Predicted\")\n",
    "#     axs[2].set_title(f\"Error, MSE: {mse:.3f}\")\n",
    "#     fig.suptitle(f\"Frame {i+1}/{td_len}\")\n",
    "\n",
    "#     fig.canvas.draw()\n",
    "#     fig.canvas.flush_events()\n",
    "\n",
    "#     # enable click-through plotting\n",
    "#     # plt.show(block=True)\n",
    "\n",
    "#     # Wait for user input to continue or close the current plot\n",
    "#     # user_input = input(\"Press enter to continue or type 'close' to close the plot and stop: \")\n",
    "#     # if user_input.lower() == 'close':\n",
    "#     #     plt.close()\n",
    "#     #     break\n",
    "\n",
    "#     # delay n seconds\n",
    "#     # plt.pause(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
