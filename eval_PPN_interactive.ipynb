{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: dataset (cross_horizontal) and dataset_weights (various/CircleV_CrossH) do not match - generalizing...\n",
      "Working on dataset: cross_horizontal\n",
      "ParaPredNet compiled...\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1, 50, 50, 3)]    0         \n",
      "                                                                 \n",
      " para_pred_net_1 (ParaPredN  (1, 1, 50, 50, 3)         110184    \n",
      " et)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110184 (430.41 KB)\n",
      "Trainable params: 110184 (430.41 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "4 PredNet layers with resolutions:\n",
      "Layer 4:  6 x 6 x 24\n",
      "Layer 3:  12 x 12 x 12\n",
      "Layer 2:  25 x 25 x 6\n",
      "Layer 1:  50 x 50 x 3\n",
      "Weights loaded successfully...\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from config import update_settings, get_settings\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "args = {\n",
    "    \"nt\": 1,\n",
    "    \"nb_epoch\": 250,\n",
    "    \"batch_size\": 1,\n",
    "    \"output_channels\": [3, 6, 12, 24],\n",
    "    \"num_P_CNN\": 1,\n",
    "    \"num_R_CLSTM\": 1,\n",
    "    \"num_passes\": 1,\n",
    "    \"pan_hierarchical\": False,\n",
    "    \"downscale_factor\": 4,\n",
    "    \"resize_images\": False,\n",
    "    \"train_proportion\": 0.7,\n",
    "    \"results_subdir\": f\"{str(datetime.now())}\",\n",
    "    \"dataset_weights\": \"various\",\n",
    "    \"data_subset_weights\": \"CircleV_CrossH\",\n",
    "    \"dataset\": \"cross_horizontal\",\n",
    "    \"data_subset\": \"cross_horizontal\",\n",
    "    \"model_choice\": \"baseline\",\n",
    "    \"system\": \"laptop\",\n",
    "    \"reserialize_dataset\": False,\n",
    "    \"output_mode\": \"Error\"\n",
    "}\n",
    "\n",
    "\n",
    "update_settings(args[\"system\"], args[\"dataset_weights\"], args[\"data_subset_weights\"], args[\"results_subdir\"])\n",
    "DATA_DIR, WEIGHTS_DIR, _, _ = get_settings()[\"dirs\"]\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import hickle as hkl\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# or '2' to filter out INFO messages too\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from data_utils import SequenceGenerator, IntermediateEvaluations, create_dataset_from_serialized_generator, config_gpus \n",
    "%matplotlib tk\n",
    "import matplotlib.pyplot as plt\n",
    "# import addcopyfighandler\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# PICK MODEL\n",
    "if args[\"model_choice\"] == \"baseline\":\n",
    "    # Predict next frame along RGB channels only\n",
    "    if not args['pan_hierarchical']:\n",
    "        from PPN_models.PPN_Baseline import ParaPredNet\n",
    "    else:\n",
    "        from PPN_models.PPN_Baseline import ParaPredNet\n",
    "        print(\"Using Pan-Hierarchical Representation\")\n",
    "elif args[\"model_choice\"] == \"cl_delta\":\n",
    "    # Predict next frame and change from current frame\n",
    "    from PPN_models.PPN_CompLearning_Delta_Predictions import ParaPredNet\n",
    "elif args[\"model_choice\"] == \"cl_recon\":\n",
    "    # Predict current and next frame\n",
    "    from PPN_models.PPN_CompLearning_Recon_Predictions import ParaPredNet\n",
    "elif args[\"model_choice\"] == \"multi_channel\":\n",
    "    # Predict next frame along Disparity, Material Index, Object Index, \n",
    "    # Optical Flow, Motion Boundaries, and RGB channels all stacked together\n",
    "    assert args[\"dataset\"] == \"monkaa\" or args[\"dataset\"] == \"driving\", \"Multi-channel model only works with Monkaa or Driving dataset\"\n",
    "    from PPN_models.PPN_Multi_Channel import ParaPredNet\n",
    "    bottom_layer_output_channels = 7 # 1 Disparity, 3 Optical Flow, 3 RGB\n",
    "    args[\"output_channels\"][0] = bottom_layer_output_channels\n",
    "else:\n",
    "    raise ValueError(\"Invalid model choice\")\n",
    "\n",
    "# where weights are loaded prior to eval\n",
    "if (args[\"dataset_weights\"], args[\"data_subset_weights\"]) in [\n",
    "    (\"rolling_square\", \"single_rolling_square\"),\n",
    "    (\"rolling_circle\", \"single_rolling_circle\"),\n",
    "]:\n",
    "    # where weights will be loaded/saved\n",
    "    weights_file = os.path.join(WEIGHTS_DIR, f\"para_prednet_\"+args[\"data_subset\"]+\"_weights.hdf5\")\n",
    "elif (args[\"dataset_weights\"], args[\"data_subset_weights\"]) in [\n",
    "    (\"all_rolling\", \"single\"),\n",
    "    (\"all_rolling\", \"multi\")\n",
    "]:\n",
    "    # where weights will be loaded/saved\n",
    "    weights_file = os.path.join(WEIGHTS_DIR, f\"para_prednet_\"+args[\"dataset_weights\"]+\"_\"+args[\"data_subset_weights\"]+\"_weights.hdf5\")\n",
    "elif args[\"dataset_weights\"] == \"various\":\n",
    "    # where weights will be loaded/saved\n",
    "    weights_file = os.path.join(WEIGHTS_DIR, f\"para_prednet_\"+args[\"dataset_weights\"]+\"_\"+args[\"data_subset_weights\"]+\"_weights.hdf5\")\n",
    "else:\n",
    "    # where weights will be loaded/saved\n",
    "    weights_file = os.path.join(WEIGHTS_DIR, f\"para_prednet_\"+args[\"dataset\"]+\"_weights.hdf5\")\n",
    "# weights_file = os.path.join(f\"/home/evalexii/Documents/Thesis/code/parallel_prednet/model_weights/{args['dataset_weights']}/{args['data_subset_weights']}\", f\"para_prednet_{args['data_subset_weights']}_weights.hdf5\")\n",
    "assert os.path.exists(weights_file), \"Weights file not found\"\n",
    "if args['dataset'] != args['dataset_weights']: \n",
    "    print(f\"WARNING: dataset ({args['dataset']}) and dataset_weights ({args['dataset_weights']}/{args['data_subset_weights']}) do not match - generalizing...\") \n",
    "else:\n",
    "    print(f\"OK: dataset ({args['dataset']}) and dataset_weights ({args['dataset_weights']}/{args['dataset_weights']}) match\") \n",
    "\n",
    "# Training parameters\n",
    "nt = args[\"nt\"]  # number of time steps\n",
    "batch_size = args[\"batch_size\"]  # 4\n",
    "output_channels = args[\"output_channels\"]\n",
    "\n",
    "# Define image shape\n",
    "if args[\"dataset\"] == \"kitti\":\n",
    "    original_im_shape = (128, 160, 3)\n",
    "    im_shape = original_im_shape\n",
    "elif args[\"dataset\"] == \"monkaa\" or args[\"dataset\"] == \"driving\":\n",
    "    original_im_shape = (540, 960, 3)\n",
    "    downscale_factor = args[\"downscale_factor\"]\n",
    "    im_shape = (original_im_shape[0] // downscale_factor, original_im_shape[1] // downscale_factor, 3)\n",
    "elif args[\"dataset\"] in [\"rolling_square\", \"rolling_circle\"]:\n",
    "    original_im_shape = (50, 100, 3)\n",
    "    downscale_factor = args[\"downscale_factor\"]\n",
    "    im_shape = (original_im_shape[0] // downscale_factor, original_im_shape[1] // downscale_factor, 3) if args[\"resize_images\"] else original_im_shape\n",
    "else:\n",
    "    original_im_shape = (50, 50, 3)\n",
    "    downscale_factor = args[\"downscale_factor\"]\n",
    "    im_shape = (original_im_shape[0] // downscale_factor, original_im_shape[1] // downscale_factor, 3) if args[\"resize_images\"] else original_im_shape\n",
    "\n",
    "print(f\"Working on dataset: {args['dataset']}\")\n",
    "\n",
    "# Create ParaPredNet\n",
    "if args[\"dataset\"] == \"kitti\":\n",
    "    # These are Kitti specific input shapes\n",
    "    inputs = (keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)))\n",
    "    PPN = ParaPredNet(args, im_height=im_shape[0], im_width=im_shape[1])  # [3, 48, 96, 192]\n",
    "    outputs = PPN(inputs)\n",
    "    PPN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "elif args[\"dataset\"] == \"monkaa\":\n",
    "    # These are Monkaa specific input shapes\n",
    "    inputs = (keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)),\n",
    "    )\n",
    "    PPN = ParaPredNet(args, im_height=im_shape[0], im_width=im_shape[1])  # [3, 48, 96, 192]\n",
    "    outputs = PPN(inputs)\n",
    "    PPN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "elif args[\"dataset\"] == \"driving\":\n",
    "    # These are driving specific input shapes\n",
    "    inputs = (keras.Input(shape=(nt, im_shape[0], im_shape[1], 1)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)),\n",
    "        keras.Input(shape=(nt, im_shape[0], im_shape[1], 3)),\n",
    "    )\n",
    "    PPN = ParaPredNet(args, im_height=im_shape[0], im_width=im_shape[1])  # [3, 48, 96, 192]\n",
    "    outputs = PPN(inputs)\n",
    "    PPN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "else:\n",
    "    # These are rolling_square specific input shapes\n",
    "    inputs = keras.Input(shape=(nt, im_shape[0], im_shape[1], 3))\n",
    "    PPN_layer = ParaPredNet(args, im_height=im_shape[0], im_width=im_shape[1])\n",
    "    PPN_layer.output_mode = \"Prediction\"\n",
    "    PPN_layer.continuous_eval = True\n",
    "    outputs = PPN_layer(inputs)\n",
    "    PPN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "resos = PPN.layers[-1].resolutions\n",
    "PPN.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "print(\"ParaPredNet compiled...\")\n",
    "PPN.build(input_shape=(None, nt) + im_shape)\n",
    "print(PPN.summary())\n",
    "num_layers = len(output_channels)  # number of layers in the architecture\n",
    "print(f\"{num_layers} PredNet layers with resolutions:\")\n",
    "for i in reversed(range(num_layers)):\n",
    "    print(f\"Layer {i+1}:  {resos[i][0]} x {resos[i][1]} x {output_channels[i]}\")\n",
    "\n",
    "# load previously saved weights\n",
    "try: \n",
    "    PPN.load_weights(weights_file)\n",
    "    print(\"Weights loaded successfully...\")\n",
    "except: \n",
    "    raise ValueError(\"Weights don't fit - exiting...\")\n",
    "\n",
    "# Load dataset - only working for animations\n",
    "test_data = hkl.load(DATA_DIR + f\"/{args['data_subset']}_train.hkl\")[0]\n",
    "td_len = test_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"140501931969856process_stream_events\"\n",
      "    while executing\n",
      "\"140501931969856process_stream_events\"\n",
      "    (\"after\" script)\n",
      "can't invoke \"event\" command: application has been destroyed\n",
      "    while executing\n",
      "\"event generate $w <<ThemeChanged>>\"\n",
      "    (procedure \"ttk::ThemeChanged\" line 6)\n",
      "    invoked from within\n",
      "\"ttk::ThemeChanged\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/500\n",
      "Iteration 3/500\n",
      "Iteration 4/500\n",
      "Iteration 5/500\n",
      "Iteration 6/500\n",
      "Iteration 7/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m fig\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mflush_events()\n\u001b[1;32m     38\u001b[0m \u001b[39m# delay n seconds\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m plt\u001b[39m.\u001b[39;49mpause(\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/PPN/lib/python3.11/site-packages/matplotlib/pyplot.py:665\u001b[0m, in \u001b[0;36mpause\u001b[0;34m(interval)\u001b[0m\n\u001b[1;32m    663\u001b[0m     canvas\u001b[39m.\u001b[39mstart_event_loop(interval)\n\u001b[1;32m    664\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 665\u001b[0m     time\u001b[39m.\u001b[39msleep(interval)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# manually initialize PPN layer states\n",
    "PPN.layers[-1].init_layer_states()\n",
    "\n",
    "# dataset_iter = iter(test_dataset)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 4))\n",
    "plt.show(block=False)\n",
    "rg_colormap = LinearSegmentedColormap.from_list('custom_cmap', [(0, 'red'), (0.5, 'black'), (1, 'green')])\n",
    "\n",
    "# test_data = np.reshape(test_data, (batch_size, td_len, im_shape[0], im_shape[1], 3))\n",
    "for i in range(td_len):\n",
    "    # ground_truth_image = next(dataset_iter)[0]\n",
    "    print(f\"Iteration {i+1}/{td_len}\")\n",
    "    ground_truth_image = np.reshape(test_data[i], (1, 1, *test_data.shape[1:]))\n",
    "    predicted_image = PPN.layers[-1](ground_truth_image)\n",
    "    error_image = ground_truth_image - predicted_image\n",
    "    error_image_grey = np.mean(error_image, axis=-1, keepdims=True)\n",
    "    mse = np.mean(error_image**2)\n",
    "\n",
    "    # clear the axes\n",
    "    axs[0].cla()\n",
    "    axs[1].cla()\n",
    "    axs[2].cla()\n",
    "\n",
    "    # print the two images side-by-side\n",
    "    axs[0].imshow(ground_truth_image[0,0,...])\n",
    "    axs[1].imshow(predicted_image[0,0,...])\n",
    "    axs[2].imshow(error_image_grey[0,0,...], cmap=rg_colormap)\n",
    "\n",
    "    # add titles\n",
    "    axs[0].set_title(\"Ground Truth\")\n",
    "    axs[1].set_title(\"Predicted\")\n",
    "    axs[2].set_title(f\"Error, MSE: {mse:.3f}\")\n",
    "    fig.suptitle(f\"Frame {i+1}/{td_len}\")\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "\n",
    "    # delay n seconds\n",
    "    plt.pause(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
